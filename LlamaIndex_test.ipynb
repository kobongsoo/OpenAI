{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab1981f-61b4-4234-a286-bfcf5a6423dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------\n",
    "# Llama index를 활요한 GPT 인덱싱 구축\n",
    "# 출처 : https://gpt-index.readthedocs.io/en/latest/getting_started/starter_example.html\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "#!pip install openai\n",
    "#!pip install llama-index\n",
    "\n",
    "# 환경변수로 OPENAI_API_KEY를 지정해야 함.\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-xxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb29015-82bc-4113-8e05-fe7d13636689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일들이 있는 폴더를 지정해서 파일로딩함.\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "folder_path = 'data'\n",
    "documents = SimpleDirectoryReader(folder_path).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b1ad3-5d99-4338-8575-523c07c2cd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "# 시맨틱 검색 예제 \n",
    "#---------------------------------------------------------------------------\n",
    "# - 기본 모델은 text-davinci-003 임.\n",
    "from llama_index import GPTSimpleVectorIndex\n",
    "index = GPTSimpleVectorIndex.from_documents(documents)\n",
    "response = index.query(\"kbs 연봉은 얼마?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c94d1e-c0e1-42bc-b57b-ca5ce8a0b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱싱을 파일로 저장 하고 불러옴.\n",
    "from llama_index import GPTSimpleVectorIndex\n",
    "index.save_to_disk('index.json')\n",
    "# Load the index from your saved index.json file\n",
    "index = GPTSimpleVectorIndex.load_from_disk('index.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045aaf83-d0d0-4d56-862b-1c43d74bb3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "# Q&A 예제\n",
    "# 참고 : https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_prompts.html\n",
    "#---------------------------------------------------------------------------\n",
    "from llama_index import QuestionAnswerPrompt, GPTSimpleVectorIndex, SimpleDirectoryReader\n",
    "\n",
    "# define custom QuestionAnswerPrompt\n",
    "query_str = \"가스공사가 감면해준 가스요금은?\"\n",
    "\n",
    "QA_PROMPT_TMPL = (\n",
    "    \"We have provided context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given this information, please answer the question: {query_str}\\n\"\n",
    ")\n",
    "\n",
    "QA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)\n",
    "\n",
    "# Build GPTSimpleVectorIndex\n",
    "index = GPTSimpleVectorIndex.from_documents(documents)\n",
    "\n",
    "response = index.query(query_str, text_qa_template=QA_PROMPT)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c3fb9f-fd65-48d8-8464-3ad15c96bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------\n",
    "# LLM 커스터마이징 예제\n",
    "#-----------------------------------------------------------------------------\n",
    "from llama_index import LLMPredictor, GPTSimpleVectorIndex, PromptHelper, ServiceContext\n",
    "from langchain import OpenAI\n",
    "\n",
    "# define LLM\n",
    "LLM = 'text-davinci-003'#'gpt-3.5-turbo'#'text-davinci-003'\n",
    "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=LLM))\n",
    "\n",
    "# define prompt helper\n",
    "# set maximum input size\n",
    "max_input_size = 4096\n",
    "# set number of output tokens\n",
    "num_output = 256\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "\n",
    "index = GPTSimpleVectorIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")\n",
    "\n",
    "response = index.query(\"동북아 오일허브 사업 생산유발효과는 얼마인가?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093e1206-0e8a-40de-a5ec-fdacbd86e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "# 요약\n",
    "# https://gpt-index.readthedocs.io/en/latest/use_cases/queries.html\n",
    "#---------------------------------------------------------------------------\n",
    "from llama_index import GPTListIndex\n",
    "index = GPTListIndex.from_documents(documents)\n",
    "query = '가스공사요금에 대한 내용을 요약해주세요.'\n",
    "response = index.query(query, response_mode=\"tree_summarize\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6c2bef-57fe-48eb-8b57-1c7225a545fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask 를 적용한 예제\n",
    "# https://github.com/logan-markewich/llama_index_starter_pack/tree/main/flask_react\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0a1a44-0e76-4b7f-91ce-652319d40a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------\n",
    "# 사용자 정의 LLM 모델 적용\n",
    "# https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html#example-using-a-custom-llm-model\n",
    "# - 에러 투성이, 속도 엄청 느림 => gpt 서버이용하는게 좋음.\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "from langchain.llms.base import LLM\n",
    "from llama_index import SimpleDirectoryReader, LangchainEmbedding, GPTListIndex, PromptHelper\n",
    "from llama_index import LLMPredictor, ServiceContext\n",
    "from transformers import pipeline\n",
    "from typing import Optional, List, Mapping, Any\n",
    "\n",
    "from os import sys\n",
    "sys.path.append('../BERT/')\n",
    "from myutils import GPU_info\n",
    "#logger = mlogging(loggername=\"test\", logfilename=\"../../log/test\")\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "# define prompt helper\n",
    "# set maximum input size\n",
    "max_input_size = 2048\n",
    "# set number of output tokens\n",
    "num_output = 256\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "    model_name = \"skt/kogpt2-base-v2\"#\"facebook/opt-iml-max-30b\"\n",
    "    pipeline = pipeline(\"text-generation\", model=model_name, device=device, model_kwargs={\"torch_dtype\":torch.bfloat16})\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        prompt_length = len(prompt)\n",
    "        response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n",
    "\n",
    "        # only return newly generated tokens\n",
    "        return response[prompt_length:]\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"name_of_model\": self.model_name}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "# define our LLM\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=CustomLLM())\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "\n",
    "# Load the your data\n",
    "folder_path = 'data'\n",
    "documents = SimpleDirectoryReader(folder_path).load_data()\n",
    "index = GPTListIndex.from_documents(documents, service_context=service_context)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222c146a-f122-4337-83fc-835e25400b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query and print response\n",
    "query = '가스공사 감면 금액은?'\n",
    "response = index.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc989b0-823d-4ef6-b3d6-0d74098c242a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
